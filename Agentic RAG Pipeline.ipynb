{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain python-dotenv tiktoken langchain-pinecone openai langchain-openai langchainhub ipykernel pandas langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd6r8E3wY8hV"
   },
   "source": [
    "# Import Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92jiBtwGBSYf"
   },
   "outputs": [],
   "source": [
    "# GLOBAL\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from uuid import uuid4\n",
    "# from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "# LANGCHAIN\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# VECTOR STORE\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# AGENTS\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import AgentExecutor, Tool, AgentType\n",
    "from langchain.agents.react.agent import create_react_agent\n",
    "from langchain import hub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyAXLKhotfpoaEY_Co69uBQUoXOQwPsP-Vc'\n",
    "os.environ['COHERE_API_KEY'] = 'vRhoY37DN64h1LJ4spzSxSrOMTry8udp2T4satyP'\n",
    "os.environ['PINECONE_API_KEY'] = '080aa9ac-3af8-4cfb-b40d-b88fe5e0ac10'\n",
    "os.environ['TAVILY_API_KEY'] = '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'\n",
    "\n",
    "\n",
    "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
    "import google.generativeai as genai\n",
    "GOOGLE_API_KEY= os.getenv('GOOGLE_API_KEY')  \n",
    "genai.configure(api_key=GOOGLE_API_KEY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyEihDI5Qzq2",
    "outputId": "088e416e-dc38-47cf-d668-2dec6b8e33e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load Documents\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"./UET_Lahore.pdf\") # Load the document\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6Oi3sTp9-c7Z"
   },
   "outputs": [],
   "source": [
    "# Define cosine similarity function\n",
    "\n",
    "def cosine_similarity(query_emb, document_emb):\n",
    "\n",
    "    # Calculate the dot product of the query and document embeddings\n",
    "    dot_product = np.dot(query_emb, document_emb)\n",
    "    # Calculate the L2 norms (magnitudes) of the query and document embeddings\n",
    "    query_norm = np.linalg.norm(query_emb)\n",
    "    document_norm = np.linalg.norm(document_emb)\n",
    "    # Calculate the cosine similarity\n",
    "    cosine_sim = dot_product / (query_norm * document_norm)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simple Example of Cosine Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlIhiUOkT0kA",
    "outputId": "25c4cea1-3342-4fa0-8457-0b1a08344d95"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "question = \"what is UET Lahore?\"\n",
    "document = \"University of Engineering and Technology (UET) Lahore is a public university located in Lahore, Punjab, Pakistan specializing in STEM subjects. It is one of the oldest and most prestigious institutions of higher learning in Pakistan. UET Lahore was established in 1921 as Mughalpura Technical College and was later renamed as Maclagan Engineering College after Sir Edward Maclagan, the then Governor of the Punjab. In 1923, the college was affiliated with the University of the Punjab for awarding a bachelor's degree in engineering. In 1962, the college was granted university status and was renamed as UET Lahore. The university offers undergraduate, postgraduate, and doctoral programs in various engineering disciplines. UET Lahore is known for its strong emphasis on research and innovation and has produced several notable alumni who have made significant contributions to the field of engineering and technology.\"\n",
    "\n",
    "# Using Google Generative AI EMbeddings Modael\n",
    "\n",
    "query_emb = embeddings.embed_query(question)\n",
    "document_emb = embeddings.embed_query(document)\n",
    "cosine_sim = cosine_similarity(query_emb, document_emb)\n",
    "print(f'Query Dimensions: {len(query_emb)}')\n",
    "print(f'Document Dimensions: {len(document_emb)}')\n",
    "print(\"Cosine Similarity:\", cosine_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pQh6fsFkiINJ"
   },
   "outputs": [],
   "source": [
    "# Splitter\n",
    "# Correct usage of RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U2OgZ6GMBrN9"
   },
   "outputs": [],
   "source": [
    "# Pinecone Initialization\n",
    "index_name = \"langchain-pinecone-malik-uet-lahore\"\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJA2iaMv9GoF"
   },
   "outputs": [],
   "source": [
    "# Create Index\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"))\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "FGJrKLSkOYJz"
   },
   "outputs": [],
   "source": [
    "# # Delete Index\n",
    "# pc.delete_index('langchain-pinecone-test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBWDX9WeSHII",
    "outputId": "9c436edb-8dde-456b-e663-024a7011c073"
   },
   "outputs": [],
   "source": [
    "# List Indexes\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l69uryiAEgJO",
    "outputId": "0cf1ed50-b0db-4a92-88f8-2174033b854e"
   },
   "outputs": [],
   "source": [
    "# Describe Index this create index \n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0_tOz8wmIZeR"
   },
   "outputs": [],
   "source": [
    "# Create Main Namespace\n",
    "splits = text_splitter.split_documents(data)\n",
    "embed = embedding= GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "db = PineconeVectorStore.from_documents(documents=splits,\n",
    "                                        embedding=embed,\n",
    "                                        index_name=index_name,\n",
    "                                        namespace=\"main\"\n",
    "                                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vectorstore of Main index and namespace means we are creating a vectorstore of the documents and their embeddings\n",
    "vectorstore = PineconeVectorStore(index_name=index_name,\n",
    "                                  namespace=\"main\",\n",
    "                                  embedding=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aji4dicUG98V",
    "outputId": "5dfeb18e-f694-485e-9955-8066b6bbb90f"
   },
   "outputs": [],
   "source": [
    "# Search for similarity\n",
    "query = \"Who is the Head of Cs Department? of UET Lahore\"\n",
    "similarity = vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "for i in range(len(similarity)):\n",
    "  print(f\"-------Result Nr. {i}-------\")\n",
    "  print(f\"Page Content: {similarity[i].page_content}\")\n",
    "  print(f\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCm3RH93LPne",
    "outputId": "a59b027f-dedf-49ce-f482-6cd111811632"
   },
   "outputs": [],
   "source": [
    "# Search for similarity with score\n",
    "\n",
    "query = \"Who is the Head of Cs Department? of UET Lahore\" \n",
    "\n",
    "similarity_with_score = vectorstore.similarity_search_with_score(query, k=4)\n",
    "\n",
    "for i in range(len(similarity_with_score)):\n",
    "  print(f\"-------Result Nr. {i}-------\")\n",
    "  print(f\"Page Content: {similarity[i].page_content}\")\n",
    "  print(f\"Score: {similarity_with_score[i][1]}\")\n",
    "  print(f\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bURbzrM4BQ0a"
   },
   "source": [
    "We can now create the namespace of the second split and check that everything has been properly created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTMbwlt0nzUA",
    "outputId": "79386452-53ad-4d47-9f5d-f073ec3f1652"
   },
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBb0lLb1aikc"
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oykb4AVEGOGr"
   },
   "source": [
    "Now that we have set up our namespaces, we can prepare our RAG pipeline. We will do so, using Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy7TxVjePRKY"
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "y0x1uf06EsXZ"
   },
   "outputs": [],
   "source": [
    "# Create vectorstore\n",
    "# embed = embedding=OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "embed = embedding= GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = PineconeVectorStore(index_name=index_name,\n",
    "                                  namespace=\"main\",\n",
    "                                  embedding=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvcx0tsIIOCS"
   },
   "source": [
    "In this retrieval step, you can chose between Open AI or Groq. For Groq, create a `GROQ_API_KEY` which allow you to use some models like llama or mistral for free. We will also add some memory, which allow to keep track of the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gjrtn8aDts1k"
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # Import the Google Generative AI model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "# Conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(memory_key='chat_history',k=5,return_messages=True)\n",
    "# Retrieval qa chain\n",
    "qa_db = RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G84UB4TdOaTz"
   },
   "source": [
    "A collection of templates can be found in the [langchain hub](https://smith.langchain.com/hub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nt_x7n23JKit",
    "outputId": "a1cace5f-829b-4e7e-8d68-67d5d7fd107c"
   },
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CmJ8bCoN7zq"
   },
   "source": [
    "Now we will replace this line:\n",
    "\n",
    "`Action: the action to take, should be one of [{tool_names}]`\n",
    "\n",
    "By this line:\n",
    "\n",
    "`Action: the action to take, should be one of [{tool_names}]. Always look first in Pinecone Document Store`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ybnrDWgibPgy"
   },
   "outputs": [],
   "source": [
    "# Set prompt template\n",
    "\n",
    "template= '''\n",
    "          Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "          {tools}\n",
    "\n",
    "          Use the following format:\n",
    "\n",
    "          Question: the input question you must answer\n",
    "          Thought: you should always think about what to do\n",
    "          Action: the action to take, should be one of [{tool_names}]. Always look first in Pinecone Document Store if not then use the tool\n",
    "          Action Input: the input to the action\n",
    "          Observation: the result of the action\n",
    "          ... (this Thought/Action/Action Input/Observation can repeat 2 times)\n",
    "          Thought: I now know the final answer\n",
    "          Final Answer: the final answer to the original input question\n",
    "\n",
    "          Begin!\n",
    "\n",
    "          Question: {input}\n",
    "          Thought:{agent_scratchpad}\n",
    "          '''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hX7Ur-nQPb6q"
   },
   "outputs": [],
   "source": [
    "# Set up tools and agent\n",
    "import os\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "tavily = TavilySearchResults(max_results=10, tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Pinecone Document Store\",\n",
    "        func = qa_db.run,\n",
    "        description = \"Use it to lookup information from the Pinecone Document Store\"\n",
    "    ),\n",
    "\n",
    "    Tool(\n",
    "        name=\"Tavily\",\n",
    "        func=tavily.run,\n",
    "        description=\"Use this to lookup information from Tavily\",\n",
    "    )\n",
    "    \n",
    "     # email sender tool\n",
    "    \n",
    "] \n",
    "agent = create_react_agent(llm,tools,prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "                        tools=tools,\n",
    "                        agent=agent,\n",
    "                        handle_parsing_errors=True,\n",
    "                        verbose=True,\n",
    "                        memory=conversational_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omGlF1tc7II_"
   },
   "source": [
    "Once everything is set up, we can start making queries and check how the agents behave in terms priorization of agent, search quality and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJcPXgySlquf",
    "outputId": "c6f7d140-61bf-499e-a0f7-e0e49bb53190"
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"what is the rola of PROF. DR. HABIB UR REHMAN at UET ?\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7fLQ6u5bPjV",
    "outputId": "10aa2121-1f1a-485e-fcc2-5fcd4412a404"
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"Which Test is pass to Take Admession in UET Lahore ?\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enUSCNdko96E",
    "outputId": "ca3c63cc-9f8a-4817-86ce-135c1b1f3414"
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"Give me The 2023-2024 academic calander ?\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czu_mPH0r4cD",
    "outputId": "682bb84a-e050-43ff-cdf3-e982868814a6"
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"Now Tell Me What is ECAT of uet Lahore  ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVNAQy_PspP0",
    "outputId": "2015b92e-e856-4a30-a55b-b09d07e46837"
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"what is CHANCELLOR'S MESSAGE ?\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lgo9hYXQmChb",
    "outputId": "7f75a347-06d1-45b0-eebe-3df91704ff5f"
   },
   "outputs": [],
   "source": [
    "conversational_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "jMiib6omZZo7"
   },
   "outputs": [],
   "source": [
    "agent_executor.memory.clear()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
